# Machine Learning Evaluation 
## Towards Reliable and Responsible AI
<img src="https://github.com/user-attachments/assets/9e76e64f-6ce7-4af3-b9cc-eae855802431" alt="9781316518861_Machine Learning Evaluation_cover" width="40%"/>

As machine learning gains widespread adoption and integration in a variety of applications, including safety- and mission-critical systems, the need for robust evaluation methods grows more urgent. This book compiles scattered information on the topic from research papers and blogs to provide a centralized resource that is accessible to students, practitioners, and researchers across the sciences.

The book examines meaningful metrics for diverse types of learning paradigms and applications, unbiased estimation methods, rigorous statistical analysis, fair training sets, and meaningful explainability, all of which are essential for building robust and reliable machine learning products. In addition to standard classification, the book discusses unsupervised learning, regression, image segmentation, and anomaly detection. It also covers topics such as industry-strength evaluation, fairness, and responsible AI. Implementations using Python and scikit-learn are available on the bookâ€™s website.

**Nathalie Japkowicz** is Professor and former Chair of the Department of Computer Science at American University, Washington, DC. She previously taught at the University of Ottawa. Her current research focuses on lifelong anomaly detection and hate speech detection. She has also researched one-class learning and the class imbalance problem extensively. She has received numerous awards, including the Test of Time and Distinguished Service awards.

**Zois Boukouvalas** is an Assistant Professor in the Department of Mathematics and Statistics at American University, Washington, DC. His research focuses on the development of interpretable multimodal machine learning algorithms, and he has been the lead principal investigator of several research grants. Through his research and teaching activities, he is creating environments that encourage and support the success of underrepresented students for entry into machine learning careers.

### **Tutorial Presentation at IEEE Big Data 2024**
In addition to the book, the insights from this work will be presented at the IEEE Big Data 2024 conference. The tutorial aims to provide an in-depth exploration of the challenges and best practices for evaluating machine learning models, focusing on industrial-strength evaluation, fairness, and responsible AI. Attendees will gain a practical understanding of how to apply these concepts to real-world machine learning systems, ensuring reliable and ethical AI development.

### [Link to Slides](https://github.com/zoisboukouvalas/MachineLearningEvaluation_TowardsReliableResponsibleAI/raw/refs/heads/main/Slides-for-BigDATA-Presentation-Machine_Learning_Evaluation_Final_Slides.pdf)


### Table of Contents
**Part I Preliminary Considerations**

1. Introduction

2. Statistics Overview

3. Machine Learning Preliminaries

4. Traditional Machine Learning Evaluation

**Part II Evaluation for Classification**

5. Metrics

6. Resampling

7. Statistical Analysis

**Part III Evaluation for Other Settings**

8. Supervised Settings Other Than Simple Classification

9. Unsupervised Learning

**Part IV Evaluation from a Practical Perspective**

10. Industrial-Strength Evaluation

11. Responsible Machine Learning

12. Conclusion

For more info about the book, please visit (https://www.cambridge.org/9781316518861)

### Acknowledgments
**Liam Spoletini** and **Sunday Okechukwu** deserve a huge thank you for their invaluable effort in creating the GitHub site that accompanies this book. Their contributions have greatly enhanced the book's usefulness.


